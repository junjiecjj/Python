
"""

最强总结，十大回归算法核心公式
https://mp.weixin.qq.com/s?__biz=MzkwNjY5NDU2OQ==&mid=2247484032&idx=1&sn=2ef69f9e0887d2858cd2846b5d84060d&chksm=c0e5d846f79251502057bf0c6e0935888d9174b37a0bfde0bacb6cd3c077d95afc204cce50a3&mpshare=1&scene=1&srcid=0726TPMnjSNHlrkOYYKuAWCy&sharer_shareinfo=c7bea49407dc008c2aab0f3721688821&sharer_shareinfo_first=c7bea49407dc008c2aab0f3721688821&exportkey=n_ChQIAhIQ0Cg%2BHOAreIQ1WLguWgxu0xKSAgIE97dBBAEAAAAAAMQkJvd6IckAAAAOpnltbLcz9gKNyK89dVj0i9wxIjZ5jhW1hQ%2FzeuEdaPBubB56HvfjaNIu%2FE2WlchMZkNvpCTKWAM8ehX0Bwn%2F%2Bn0Gllsm1QmsgrCgxtr7auz5%2FZKAtMdC%2BWPh0qGcVI7LOlVflIuxv1HlaOjVy6jXWaHhVTjzET33%2B55pB4Vi85lJYH2hjpFiRQmpQkK32nKXucmV8ThIauCNGyBGzvLKX8Uul41hAhyYV2Q9gXRmFzq4lS7KD2Kkl2SKZ3QHtEMQmNuaaVtcuJu7l%2B0wE3nJzs4AXKhjxu%2FH4Ihp8y5Qfeh1k9HAv%2B0VjgkSYS17KtJqySCICovTCcLcNCU%3D&acctmode=0&pass_ticket=akRwruQaQHbUHlYyJoWpqkdyRpyRViPQjUIUZCUuB61UVwuHmShJqfmT44Qc%2BC9n&wx_header=0#rd

通透！十大回归算法全总结！
https://mp.weixin.qq.com/s?__biz=Mzk0MjUxMzg3OQ==&mid=2247486033&idx=1&sn=1c6ad89d9cb7923dfc58554184d7dc1f&chksm=c2c341e9f5b4c8ff84123e2159f8511587cceee966844b48b38c643f70ff3d0c8f47a0ff47a7&mpshare=1&scene=1&srcid=0726ta8LXP8xzLdqpJhhZiB3&sharer_shareinfo=40f1a55d93656485fd0ae74dac3ce470&sharer_shareinfo_first=40f1a55d93656485fd0ae74dac3ce470&exportkey=n_ChQIAhIQJ4ZsK%2FLMOR8wyFoB2fY9hBKQAgIE97dBBAEAAAAAAOGLIrpxJpEAAAAOpnltbLcz9gKNyK89dVj0%2FKyDTcgdn0zuf0vguhOdv6Ib0SELpfWt93ag7x4GLmyW9KX3f2vqQ4OnB3dDaHrrFnT4mBr9UXIjgNuoz6avloTSZKWhzWTTLBxnitGmTu085v7kcF%2FoL4kmYo9R57%2FqXVN6n9V%2FR8HdfLAHvCiF4Pmxoxk4wB92Oe0bvB5kb8rZl3uw4ydATqEUu3Z%2FXEJnxcAWCF8X6TOHp6UbYJjKy2IVmFEoYBSWTze%2F1zIYVg7e%2B8rLuHIoBm9XVjrhpbf8wzXlrM9%2BOQTLVfYSUCb20u3StWnVvTFDRjBnWPOEbA1veLvKdxdDXhTX&acctmode=0&pass_ticket=HClY0DMZAVhWlVUAS38Y60hCN09JIO9gbO83ddMztht%2FwZF3fZtFu1wldf4L6w05&wx_header=0#rd

最强总结，十大回归算法
https://mp.weixin.qq.com/s?__biz=MzkwNjY5NDU2OQ==&mid=2247484158&idx=1&sn=49656b0ca74b7ae77e6cb2937c21bda8&chksm=c0e5d838f792512e7f75de7132da37fe6b0fa9cb9b88d63a8d51c48b6cc4887f4a2e9a5efff4&mpshare=1&scene=24&srcid=0726X7VLbaYJck1popaMp0xg&sharer_shareinfo=522a1db1b91390f858bdc17bf78eb673&sharer_shareinfo_first=522a1db1b91390f858bdc17bf78eb673&exportkey=n_ChQIAhIQpCyLHQS%2F5vQCZve8C4PcORKfAgIE97dBBAEAAAAAACPtA%2F6ALIYAAAAOpnltbLcz9gKNyK89dVj0g1UjtUuokWEGXG%2BN4gATITUmtfK0N%2FRxE%2FgxHBZcWiylcKaQpAw71ochOhthdA7gqhix2MsIGHKdo3xlYtn2tod%2BMK3u5n%2B0A7jgR5DXnhyh46vlaiRvFu03IZzLK6Umr8UDj5FbZC1ez5f2t7IyM4rBs4vEnEKFEKdjHW9GJvHqMrFpiiOD85D3duLUC9MRcrzHQyKmh16Rh%2BP%2Fgu3pB4J2ga1iYp%2FZ0E84mleSPpUDc3e3TSnb%2B8fACAdcu8hn3vgZarwue7UT9gTOxJ5wFHhFNcad9ioz1ufLQ1PL%2BLbrefllhdN0zwMHael37Jv1hI%2BvZWMJmt%2F8&acctmode=0&pass_ticket=A%2Bavg7BGsevCLBiWSqA7MW3PxXyPNjtSWN7QDwNYJCRhUbYkAE8xL4N8WneKTHX%2F&wx_header=0#rd


https://mp.weixin.qq.com/s?__biz=MzkwNjY5NDU2OQ==&mid=2247485441&idx=1&sn=13fb6ffa27ebe9664d69e21ebfc501eb&chksm=c0e5d2c7f7925bd104a6f0f5fba1899650786cf03eb191c1f5f564caa83d7b96dd7e3b04fd10&mpshare=1&scene=1&srcid=0803j4gftq59jeAxRMjTZi1y&sharer_shareinfo=e1071559ef8088067aa937e834a62074&sharer_shareinfo_first=e1071559ef8088067aa937e834a62074&exportkey=n_ChQIAhIQRd1zR798PYwvTREHaaa9gRKfAgIE97dBBAEAAAAAAG02McEEMIYAAAAOpnltbLcz9gKNyK89dVj0xNwDFBoGNay6B7DGzbeNxBOq7AtPtfNSECwRJ6zi2C7HMVy44UUDB5fXjzkJfpqZD%2F2zzpycZPc%2FU%2FRjfw2dSz9LFpc2pSucgBFx0B5JFrlBh0JareJiIkGUshSKsK7EMooLeulVl7rcG5512qHKUeqOoRHvxeRt5%2BWGoXPLAHbvS7XLlqdOfWjWnEYB940LI2dUpHc1JUlX38ua2wkqmvX4sSfXeBJDbKvUZVNPtwJxjCq46EpD6CXGYIMWS13SxPnlsGNcV1LLKXqfzSb7AQBD9S%2FRZOQ7S4pzNR1GK0gOchSBkrK3aatBxx%2FIZmH%2Bq8RA97M%2B6LID&acctmode=0&pass_ticket=3XOVYBHCqjJTf%2BiHx%2BYcsMq%2BpDBNiF6pJCoeGUGQPaL7s%2FHErxL24DbU6M6ZK%2B2g&wx_header=0#rd


"""
#%% 1. 线性回归 (Linear Regression)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# 生成随机数据
np.random.seed(0)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# 线性回归模型
lin_reg = LinearRegression()
lin_reg.fit(X, y)
y_pred = lin_reg.predict(X)

# 绘图
plt.scatter(X, y, color='blue', label='Data points')
plt.plot(X, y_pred, color='red', label='Regression line')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression')
plt.legend()
plt.show()




#%% 2. 岭回归 (Ridge Regression)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge

# 生成随机数据
np.random.seed(0)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# 岭回归模型
ridge_reg = Ridge(alpha=1, solver="cholesky")
ridge_reg.fit(X, y)
y_pred = ridge_reg.predict(X)

# 绘图
plt.scatter(X, y, color='blue', label='Data points')
plt.plot(X, y_pred, color='red', label='Ridge regression line')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Ridge Regression')
plt.legend()
plt.show()




#%% 3. lasso回归 (Lasso Regression)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Lasso

# 生成随机数据
np.random.seed(0)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# 拉索回归模型
lasso_reg = Lasso(alpha=0.1)
lasso_reg.fit(X, y)
y_pred = lasso_reg.predict(X)

# 绘图
plt.scatter(X, y, color='blue', label='Data points')
plt.plot(X, y_pred, color='red', label='Lasso regression line')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Lasso Regression')
plt.legend()
plt.show()




#%% 4. 弹性网络回归 (Elastic Net Regression)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import ElasticNet

# 生成随机数据
np.random.seed(0)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# 弹性网络回归模型
elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic_net.fit(X, y)
y_pred = elastic_net.predict(X)

# 绘图
plt.scatter(X, y, color='blue', label='Data points')
plt.plot(X, y_pred, color='red', label='Elastic Net regression line')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Elastic Net Regression')
plt.legend()
plt.show()




#%% 5. 多项式回归 (Polynomial Regression)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# 生成随机数据
np.random.seed(0)
X = 6 * np.random.rand(100, 1) - 3
y = 0.5 * X**2 + X + 2 + np.random.randn(100, 1)

# 多项式特征变换
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)

# 线性回归模型
lin_reg = LinearRegression()
lin_reg.fit(X_poly, y)
y_pred = lin_reg.predict(X_poly)

# 绘图
plt.scatter(X, y, color='blue', label='Data points')
plt.plot(X, y_pred, color='red', label='Polynomial regression line')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Polynomial Regression')
plt.legend()
plt.show()


#%% 6. 支持向量回归 (Support Vector Regression, SVR)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVR

# 生成随机数据
np.random.seed(0)
X = 6 * np.random.rand(100, 1) - 3
y = 0.5 * X**2 + X + 2 + np.random.randn(100, 1)

# 支持向量回归模型
svr = SVR(kernel='rbf', C=100, epsilon=0.1)
svr.fit(X, y.ravel())
y_pred = svr.predict(X)

# 绘图
plt.scatter(X, y, color='blue', label='Data points')
plt.plot(X, y_pred, color='red', label='SVR regression line')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Support Vector Regression')
plt.legend()
plt.show()


#%% 7. 决策树回归 (Decision Tree Regression)

# 导入所需的库
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor, plot_tree

# 创建一个虚拟数据集
np.random.seed(0)
X = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(X).ravel()
y[::5] += 3 * (0.5 - np.random.rand(16))  # 添加噪声

# 训练回归模型
regressor = DecisionTreeRegressor(max_depth=5)
regressor.fit(X, y)

# 绘制决策树图像
plt.figure(figsize=(10, 8))
plot_tree(regressor, filled=True)
plt.title("Decision Tree Regression")
plt.show()

#%% 8. 随机森林回归 (Random Forest Regression)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.ensemble import RandomForestRegressor

X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)

# 训练随机森林回归模型
regressor = RandomForestRegressor(n_estimators=100, random_state=42)
regressor.fit(X, y)

# 预测结果
X_test = np.linspace(-3, 3, 100).reshape(-1, 1)
y_pred = regressor.predict(X_test)

# 绘制随机森林中的一棵决策树图像
estimator = regressor.estimators_[0]

plt.figure(figsize=(10, 8))
plt.scatter(X, y, color="b", s=30, marker="o", label="training data")
plt.plot(X_test, y_pred, color="r", label="predictions")
plt.title("Random Forest Regression")
plt.xlabel("Feature")
plt.ylabel("Target")
plt.legend(loc="upper left")

# 可视化一棵决策树
plt.figure(figsize=(15, 10))
from sklearn.tree import plot_tree
plot_tree(estimator, filled=True, feature_names=['Feature'])
plt.title("Example Decision Tree from Random Forest")
plt.show()

#%% 9. 梯度提升回归 (Gradient Boosting Regression)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingRegressor

# 创建一个非线性函数作为模拟数据
def true_function(x):
    return np.sin(x) + np.sin(10 * x)

# 生成模拟数据
np.random.seed(0)
X = np.sort(5 * np.random.rand(200, 1), axis=0)
y = true_function(X).ravel()
dy = 0.5 + 1.0 * np.random.rand(200)
y += np.random.normal(0, dy)

# 拟合梯度提升回归模型
est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=0, loss='squared_error')
est.fit(X, y,  )

# 生成预测数据
X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]
y_pred = est.predict(X_test)

# 绘制结果
plt.figure(figsize=(10, 6))
plt.errorbar(X.ravel(), y, dy, fmt='o', alpha=0.5, label='Observations')
plt.plot(X_test, true_function(X_test), label='True function', color='blue')
plt.plot(X_test, y_pred, '--', color='navy', label='Predicted function')
plt.fill_between(X_test.ravel(), y_pred - est.estimators_[0][0].predict(X_test), y_pred + est.estimators_[0][0].predict(X_test), color='red', alpha=0.6, label='Uncertainty')
plt.title('Gradient Boosting Regression')
plt.legend(loc='upper left')
plt.xlabel('X')
plt.ylabel('y')
plt.show()


#%% 10. 贝叶斯回归 (Bayesian Regression)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import BayesianRidge

# 创建一个非线性函数作为模拟数据
def true_function(x):
    return np.sin(x) + np.sin(2 * x) + np.sin(3 * x)

# 生成模拟数据
np.random.seed(0)
X = np.sort(5 * np.random.rand(200, 1), axis=0)
y = true_function(X).ravel()
dy = 0.5 + 1.0 * np.random.rand(200)
y += np.random.normal(0, dy)

# 贝叶斯线性回归模型
poly = PolynomialFeatures(degree=5)
X_poly = poly.fit_transform(X)
clf = BayesianRidge(compute_score=True)
clf.fit(X_poly, y)

# 生成预测数据
X_test = np.linspace(0, 5, 100)[:, np.newaxis]
y_pred, y_std = clf.predict(poly.transform(X_test), return_std=True)

# 绘制结果
plt.figure(figsize=(10, 6))
plt.errorbar(X.ravel(), y, dy, fmt='o', alpha=0.5, label='Observations')
plt.plot(X_test, true_function(X_test), color='green', label='True function')
plt.plot(X_test, y_pred, '--', color='navy', label='Predicted function')
plt.fill_between(X_test.ravel(), y_pred - y_std, y_pred + y_std, color='red', alpha=0.6, label='Uncertainty')
plt.title('Bayesian Regression with Polynomial Features')
plt.legend(loc='upper left')
plt.xlabel('X')
plt.ylabel('y')
plt.show()


#%% 1. 线性回归 (Linear Regression)
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# 生成虚拟数据
np.random.seed(0)
n_samples = 1000
X = np.random.rand(n_samples, 1) * 10  # 特征变量
y = 3 * X.squeeze() + np.random.randn(n_samples) * 2  # 目标变量（线性关系 + 噪声）

# 将数据集分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# 创建并训练线性回归模型
model = LinearRegression()
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)

# 计算回归模型性能指标
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.2f}")
print(f"R^2 Score: {r2:.2f}")

# 可视化

# 创建一个数据框用于可视化
df = pd.DataFrame({'X': X.squeeze(), 'y': y})
df_test = pd.DataFrame({'X': X_test.squeeze(), 'y': y_test, 'y_pred': y_pred})

# 散点图及回归线
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
sns.scatterplot(x='X', y='y', data=df, label='Actual Data', color='blue')
sns.lineplot(x='X', y='y', data=df_test, color='red', label='Regression Line')
plt.xlabel('Feature Variable')
plt.ylabel('Target Variable')
plt.title('Scatter Plot with Regression Line')
plt.legend()

# 残差图
residuals = y_test - y_pred
plt.subplot(1, 2, 2)
sns.scatterplot(x=y_pred, y=residuals, color='green')
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residual Plot')

plt.tight_layout()
plt.show()


#%% 2. 岭回归 (Ridge Regression)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures

# 生成虚拟数据集
np.random.seed(42)
X = np.sort(5 * np.random.rand(1000, 1), axis=0)
y = np.sin(X).ravel() + np.random.normal(0, 0.2, X.shape[0])

# 数据集分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 多项式特征
poly = PolynomialFeatures(degree=5)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

# 岭回归模型
ridge = Ridge(alpha=1.0)
ridge.fit(X_train_poly, y_train)

# 预测
y_train_pred = ridge.predict(X_train_poly)
y_test_pred = ridge.predict(X_test_poly)

# 图1: 原始数据与岭回归拟合曲线
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='gray', label='Data')
X_plot = np.linspace(0, 5, 100).reshape(-1, 1)
X_plot_poly = poly.transform(X_plot)
y_plot = ridge.predict(X_plot_poly)
plt.plot(X_plot, y_plot, color='red', label='Ridge Regression Fit')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Ridge Regression Fit')
plt.legend()
plt.show()

# 图2: 训练集和测试集的预测值与实际值对比
plt.figure(figsize=(10, 6))
plt.scatter(y_train, y_train_pred, color='blue', label='Train data')
plt.scatter(y_test, y_test_pred, color='green', label='Test data')
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2, label='Ideal fit')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted')
plt.legend()
plt.show()

# 图3: 岭回归系数随正则化参数变化情况
alphas = np.logspace(-6, 6, 200)
coefs = []
for a in alphas:
    ridge = Ridge(alpha=a, fit_intercept=False)
    ridge.fit(X_train_poly, y_train)
    coefs.append(ridge.coef_)

plt.figure(figsize=(10, 6))
ax = plt.gca()
ax.plot(alphas, coefs)
ax.set_xscale('log')
plt.xlabel('alpha')
plt.ylabel('coefficients')
plt.title('Ridge coefficients as a function of the regularization')
plt.axis('tight')
plt.show()

# 计算误差和R^2
mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'Training set MSE: {mse_train}')
print(f'Testing set MSE: {mse_test}')
print(f'Training set R^2: {r2_train}')
print(f'Testing set R^2: {r2_test}')




#%% 3. 拉索回归 (Lasso Regression)
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error, r2_score

# 生成虚拟数据集
np.random.seed(0)
n_samples, n_features = 1000, 10
X = np.random.randn(n_samples, n_features)
true_coef = np.array([1.5, -2.0, 0, 0, 0, 0, 3.0, 0, 0, -4.0])
y = np.dot(X, true_coef) + np.random.normal(size=n_samples)

# 拆分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 拉索回归
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)

# 预测
y_train_pred = lasso.predict(X_train)
y_test_pred = lasso.predict(X_test)

# 特征权重的条形图
plt.figure(figsize=(10, 6))
plt.bar(range(n_features), lasso.coef_)
plt.xlabel('Features')
plt.ylabel('Coefficients')
plt.title('Lasso Regression Coefficients')
plt.xticks(range(n_features), [f'Feature {i}' for i in range(n_features)])
plt.show()

# 预测值与真实值的散点图
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_test_pred, edgecolors=(0, 0, 0))
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Lasso Regression: Actual vs Predicted')
plt.show()

# 输出模型性能指标
print(f"Mean Squared Error (Train): {mean_squared_error(y_train, y_train_pred)}")
print(f"Mean Squared Error (Test): {mean_squared_error(y_test, y_test_pred)}")
print(f"R^2 Score (Train): {r2_score(y_train, y_train_pred)}")
print(f"R^2 Score (Test): {r2_score(y_test, y_test_pred)}")



#%% 4. 弹性网络回归 (Elastic Net Regression)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import ElasticNetCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# 生成虚拟数据集
np.random.seed(42)
n_samples, n_features = 1000, 3
X = np.random.randn(n_samples, n_features)
coef = 3 * np.random.randn(n_features)
# 增加噪声
y = np.dot(X, coef) + np.random.normal(0, 0.5, size=n_samples)

# 分割数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 拟合弹性网络回归模型
regr = ElasticNetCV(cv=5, random_state=42)
regr.fit(X_train, y_train)

# 输出特征系数
print("Coefficients: ", regr.coef_)
print("Intercept: ", regr.intercept_)

# 预测
y_pred = regr.predict(X_test)

# 第一个图：特征系数随正则化参数的变化图
plt.figure(figsize=(10, 6))
alphas = regr.alphas_
coefs = regr.path(X_train, y_train, l1_ratio=regr.l1_ratio_)[1]
for coef_l in coefs:
    plt.plot(alphas, coef_l)
plt.xscale('log')
plt.xlabel('Alpha')
plt.ylabel('Coefficients')
plt.title('Coefficients as a function of the regularization')
plt.axis('tight')
plt.show()

# 第二个图：实际值与预测值的比较散点图
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue', edgecolor='k', alpha=0.7)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', lw=2)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted values')
plt.show()

# 计算 R^2 分数
r2 = r2_score(y_test, y_pred)
print(f'R^2 score: {r2:.2f}')



#%% 5. 多项式回归 (Polynomial Regression)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# 生成虚拟数据集
np.random.seed(0)
X = np.random.rand(1000, 1) * 10  # 生成 0 到 10 之间的随机数
y = 2 + 1.5 * X + X**2 + np.random.randn(1000, 1) * 10  # 添加二次项和噪声

# 将数据集拆分为训练集和测试集
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# 多项式特征变换
poly = PolynomialFeatures(degree=2)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

# 线性回归模型
model = LinearRegression()
model.fit(X_train_poly, y_train)

# 预测
y_train_pred = model.predict(X_train_poly)
y_test_pred = model.predict(X_test_poly)

# 计算误差
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)

# 打印误差
print(f'Training RMSE: {train_rmse}, R2: {train_r2}')
print(f'Testing RMSE: {test_rmse}, R2: {test_r2}')

# 图1：数据点及其多项式回归拟合曲线
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', label='Data Points')
plt.plot(np.sort(X, axis=0), model.predict(poly.transform(np.sort(X, axis=0))), color='red', linewidth=2, label='Polynomial Fit')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Polynomial Regression Fit')
plt.legend()
plt.show()

# 图2：残差图
plt.figure(figsize=(10, 6))
plt.scatter(y_train, y_train - y_train_pred, color='blue', label='Training Data')
plt.scatter(y_test, y_test - y_test_pred, color='red', label='Test Data')
plt.hlines(0, min(y_train), max(y_train), colors='black', linestyles='dashed')
plt.xlabel('Actual y')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.legend()
plt.show()

# 图3：实际值与预测值的散点图
plt.figure(figsize=(10, 6))
plt.scatter(y_train, y_train_pred, color='blue', label='Training Data')
plt.scatter(y_test, y_test_pred, color='red', label='Test Data')
plt.plot([min(y), max(y)], [min(y), max(y)], color='black', linewidth=2, linestyle='dashed')
plt.xlabel('Actual y')
plt.ylabel('Predicted y')
plt.title('Actual vs Predicted')
plt.legend()
plt.show()



#%% 6. 支持向量回归 (Support Vector Regression, SVR)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score

# 创建虚拟数据集
np.random.seed(42)
X = np.sort(5 * np.random.rand(1000, 1), axis=0)
y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])

# 拟合SVR模型
svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)
svr_rbf.fit(X, y)
y_pred = svr_rbf.predict(X)

# 计算残差
residuals = y - y_pred

# 绘制原始数据和回归曲线
plt.figure(figsize=(14, 6))

plt.subplot(2, 2, 1)
plt.scatter(X, y, color='darkorange', label='Data')
plt.plot(X, y_pred, color='navy', lw=2, label='SVR model')
plt.xlabel('Data')
plt.ylabel('Target')
plt.title('SVR Regression')
plt.legend()

# 绘制残差图
plt.subplot(2, 2, 2)
plt.scatter(X, residuals, color='red', edgecolor='w', label='Residuals')
plt.axhline(y=0, color='black', linestyle='--')
plt.xlabel('Data')
plt.ylabel('Residual')
plt.title('Residuals of SVR')

# 绘制支持向量
plt.subplot(2, 2, 3)
plt.scatter(X, y, color='darkorange', label='Data')
plt.scatter(X[svr_rbf.support_], y[svr_rbf.support_], facecolors='none', edgecolors='k',  s=100, label='Support Vectors')
plt.xlabel('Data')
plt.ylabel('Target')
plt.title('Support Vectors')
plt.legend()

# 绘制残差直方图
plt.subplot(2, 2, 4)
plt.hist(residuals, bins=20, color='blue', edgecolor='black')
plt.xlabel('Residual')
plt.ylabel('Frequency')
plt.title('Histogram of Residuals')

plt.tight_layout()
plt.show()

# 打印模型评估指标
mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')






#%% 7. 决策树回归 (Decision Tree Regression)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from mpl_toolkits.mplot3d import Axes3D

# 生成虚拟数据
np.random.seed(42)
n_samples = 1000
temperature = np.random.uniform(20, 1000, n_samples)
time = np.random.uniform(1, 10, n_samples)
reaction_speed = 0.05 * temperature**1.5 - 0.5 * time**2 + np.random.normal(0, 5, n_samples)

# 创建DataFrame
data = pd.DataFrame({'Temperature': temperature, 'Time': time, 'ReactionSpeed': reaction_speed})

# 特征和目标
X = data[['Temperature', 'Time']]
y = data['ReactionSpeed']

# 决策树回归模型
model = DecisionTreeRegressor(max_depth=5)
model.fit(X, y)

# 预测
X_test = np.linspace(20, 100, 100)
Y_test = np.linspace(1, 10, 100)
X_test, Y_test = np.meshgrid(X_test, Y_test)
Z_pred = model.predict(np.c_[X_test.ravel(), Y_test.ravel()]).reshape(X_test.shape)

# 图形1: 3D 表面图
fig = plt.figure(figsize=(12, 6))
ax = fig.add_subplot(121, projection='3d')
ax.plot_surface(X_test, Y_test, Z_pred, cmap='viridis', edgecolor='none')
ax.set_xlabel('Temperature')
ax.set_ylabel('Time')
ax.set_zlabel('Predicted Reaction Speed')
ax.set_title('3D Surface Plot of Predicted Reaction Speed')

# 图形2: 预测 vs 实际
ax2 = fig.add_subplot(122)
y_pred = model.predict(X)
ax2.scatter(y, y_pred, c='blue', marker='o', edgecolor='w', s=70)
ax2.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)
ax2.set_xlabel('Actual Reaction Speed')
ax2.set_ylabel('Predicted Reaction Speed')
ax2.set_title('Predicted vs Actual Reaction Speed')

plt.tight_layout()
plt.show()




#%% 8. 随机森林回归 (Random Forest Regression)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import seaborn as sns
import pandas as pd

# 生成虚拟数据集
np.random.seed(42)
X = np.random.rand(1000, 1) * 10
y = np.sin(X).ravel() + np.random.normal(0, 0.5, X.shape[0])

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练随机森林回归模型
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# 预测
y_pred = rf.predict(X_test)

# 模型评估
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'MSE: {mse:.4f}')
print(f'R^2: {r2:.4f}')

# 创建 DataFrame 以便绘图
df = pd.DataFrame({'X': X_test.flatten(), 'y_true': y_test, 'y_pred': y_pred})

# 绘制实际值 vs 预测值
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.scatter(X_test, y_test, color='blue', label='Actual')
plt.scatter(X_test, y_pred, color='red', alpha=0.5, label='Predicted')
plt.title('Actual vs Predicted')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()

# 绘制残差图
plt.subplot(1, 2, 2)
residuals = y_test - y_pred
plt.scatter(X_test, residuals, color='purple')
plt.hlines(0, xmin=X_test.min(), xmax=X_test.max(), colors='gray', linestyles='dashed')
plt.title('Residual Plot')
plt.xlabel('X')
plt.ylabel('Residuals')
plt.tight_layout()
plt.show()

# 特征重要性
feature_importances = rf.feature_importances_
plt.figure(figsize=(8, 6))
plt.bar(range(len(feature_importances)), feature_importances, tick_label=['Feature 1'])
plt.title('Feature Importances')
plt.show()

# 绘制预测 vs 实际值的密度图
plt.figure(figsize=(8, 6))
sns.kdeplot(y_test, label='Actual', color='blue', fill=True, alpha=0.3)
sns.kdeplot(y_pred, label='Predicted', color='red', fill=True, alpha=0.3)
plt.title('Density Plot of Actual vs Predicted Values')
plt.xlabel('y')
plt.ylabel('Density')
plt.legend()
plt.show()

# 使用热图显示预测误差分布
plt.figure(figsize=(8, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Heatmap')
plt.show()






#%% 9. 梯度提升回归 (Gradient Boosting Regression)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# 设置随机种子
np.random.seed(42)

# 生成虚拟数据
n_samples = 1000
area = np.random.normal(loc=100, scale=20, size=n_samples)  # 房屋面积
rooms = np.random.randint(1, 8, size=n_samples)  # 房间数量
floors = np.random.randint(1, 3, size=n_samples)  # 楼层数量 (1或2)
age = np.random.randint(0, 50, size=n_samples)  # 房龄
location = np.random.choice(['downtown', 'suburb', 'rural'], size=n_samples)  # 位置

# 生成房价，假设与以上特征有一定关系
price = ( 20000 + 300 * area + 5000 * rooms + 10000 * floors - 200 * age +
         np.where(location == 'downtown', 50000, 0) +
         np.where(location == 'suburb', 20000, 0) +
         np.random.normal(0, 10000, size=n_samples)  # 添加噪声
         )

# 创建 DataFrame
data = pd.DataFrame({
    'area': area,
    'rooms': rooms,
    'floors': floors,
    'age': age,
    'location': location,
    'price': price })

# 独热编码
data = pd.get_dummies(data, columns=['location'], drop_first=True)

# 分割数据集
X = data.drop('price', axis=1)
y = data['price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建和训练模型
model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
model.fit(X_train, y_train)

# 预测
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# 计算误差
mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f"训练集均方误差: {mse_train:.2f}")
print(f"测试集均方误差: {mse_test:.2f}")
print(f"训练集R2: {r2_train:.2f}")
print(f"测试集R2: {r2_test:.2f}")

# 绘制预测值 vs 真实值 散点图
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_test_pred, alpha=0.6, color='b', label='Predicted vs Actual')
plt.plot([y.min(), y.max()], [y.min(), y.max()], '--', color='r', label='Ideal Fit')
plt.xlabel('anctual')
plt.ylabel('predict')
plt.title('predict vs predict')
plt.legend()
plt.show()

# 获取特征重要性
feature_importances = model.feature_importances_
features = X.columns

# 绘制特征重要性条形图
plt.figure(figsize=(12, 6))
sns.barplot(x=feature_importances, y=features)
plt.xlabel('特征重要性')
plt.ylabel('特征')
plt.title('特征重要性分析')
plt.show()



#%% 10. K近邻回归 (K-Nearest Neighbors Regression, KNN)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 生成虚拟数据集
np.random.seed(0)
X = np.sort(5 * np.random.rand(800, 1), axis=0)
y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])

# 将数据集分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# 创建并训练K近邻回归模型
k = 5  # 选择K值
model = KNeighborsRegressor(n_neighbors=k)
model.fit(X_train, y_train)

# 进行预测
y_pred = model.predict(X_test)

# 计算均方误差
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

# 绘制图形

# 1. 绘制散点图和回归预测图
plt.figure(figsize=(14, 6))

# 散点图
plt.subplot(1, 2, 1)
plt.scatter(X_train, y_train, color='blue', label='Training data')
plt.scatter(X_test, y_test, color='red', label='Test data')
plt.title('Scatter Plot of Training and Test Data')
plt.xlabel('Feature')
plt.ylabel('Target')
plt.legend()

# 回归预测图
plt.subplot(1, 2, 2)
X_range = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
y_range_pred = model.predict(X_range)
plt.plot(X_range, y_range_pred, color='green', label='KNN Regression')
plt.scatter(X_train, y_train, color='blue', label='Training data')
plt.scatter(X_test, y_test, color='red', label='Test data')
plt.title('KNN Regression Curve')
plt.xlabel('Feature')
plt.ylabel('Target')
plt.legend()

plt.show()









#%%



















#%%



















#%%



















#%%



















#%%



















#%%



















#%%



















#%%



















#%%



















#%%



















#%%



















